# -*- coding: utf-8 -*-
"""
Created on Thu Nov  6 09:59:01 2025

@author: ypan1
"""

import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from io import StringIO
import base64

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ™ºèƒ½ç¨³å¥æ ‡å‡†å·®åˆ†æå™¨",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åº”ç”¨æ ‡é¢˜å’Œæè¿°
st.title("ğŸ“Š æ™ºèƒ½ç¨³å¥æ ‡å‡†å·®åˆ†æå™¨")
st.markdown("""
æ­¤åº”ç”¨ä½¿ç”¨Huber's M-estimatorè‡ªåŠ¨åˆ†ææ•°æ®çš„ç¨³å¥æ ‡å‡†å·®ç‰¹æ€§ï¼Œèƒ½å¤Ÿæ™ºèƒ½è¯†åˆ«æ•°æ®ç±»å‹å¹¶é€‰æ‹©æœ€ä¼˜å‚æ•°ã€‚
æ”¯æŒ**é«˜åº¦é›†ä¸­æ•°æ®**ã€**ä¸­ç­‰é›†ä¸­æ•°æ®**å’Œ**æ­£å¸¸åˆ†å¸ƒæ•°æ®**çš„è‡ªåŠ¨è¯†åˆ«ä¸å¤„ç†ã€‚
""")

# ä¾§è¾¹æ é…ç½®
st.sidebar.header("é…ç½®å‚æ•°")
st.sidebar.markdown("è°ƒæ•´Huber M-estimatorçš„å‚æ•°è®¾ç½®ï¼š")

# Huberå‚æ•°è®¾ç½®
c_value = st.sidebar.slider(
    "Huberå‚æ•° c", 
    min_value=1.0, 
    max_value=2.0, 
    value=1.345, 
    step=0.01,
    help="è¾ƒå°çš„cå€¼å¯¹å¼‚å¸¸å€¼æ›´æ•æ„Ÿï¼Œè¾ƒå¤§çš„cå€¼æ›´æ¥è¿‘ä¼ ç»Ÿæ ‡å‡†å·®"
)

# æ•°æ®è¾“å…¥æ–¹å¼é€‰æ‹©
input_method = st.radio("é€‰æ‹©æ•°æ®è¾“å…¥æ–¹å¼:", 
                       ["ä¸Šä¼ CSVæ–‡ä»¶", "ç›´æ¥è¾“å…¥æ•°æ®", "ä½¿ç”¨ç¤ºä¾‹æ•°æ®"])

class RobustStdAnalyzer:
    """ç¨³å¥æ ‡å‡†å·®åˆ†æå™¨"""
    
    def __init__(self):
        self.data_type = None
        self.analysis_results = {}
    
    def analyze_data_characteristics(self, data):
        """åˆ†ææ•°æ®ç‰¹å¾"""
        n = len(data)
        mean_val = np.mean(data)
        median_val = np.median(data)
        std_val = np.std(data)
        mad_val = np.median(np.abs(data - median_val))
        
        # æ•°æ®åˆ†å¸ƒç‰¹å¾
        unique_vals, counts = np.unique(data, return_counts=True)
        max_count = np.max(counts)
        concentration_ratio = max_count / n
        
        # å¼‚å¸¸å€¼æ£€æµ‹
        Q1 = np.percentile(data, 25)
        Q3 = np.percentile(data, 75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = data[(data < lower_bound) | (data > upper_bound)]
        outlier_ratio = len(outliers) / n
        
        # åˆ¤æ–­æ•°æ®ç±»å‹
        if concentration_ratio > 0.8 and mad_val == 0:
            data_type = "é«˜åº¦é›†ä¸­æ•°æ®"
        elif concentration_ratio > 0.6 and outlier_ratio < 0.1:
            data_type = "ä¸­ç­‰é›†ä¸­æ•°æ®"
        else:
            data_type = "æ­£å¸¸åˆ†å¸ƒæ•°æ®"
            
        characteristics = {
            'n': n, 'mean': mean_val, 'median': median_val, 'std': std_val,
            'mad': mad_val, 'concentration_ratio': concentration_ratio,
            'outlier_ratio': outlier_ratio, 'data_type': data_type,
            'unique_values_count': len(unique_vals), 'IQR': IQR
        }
        
        return characteristics
    
    def huber_robust_std(self, data, c=1.345, tol=1e-6, max_iter=100):
        """Huber M-estimatorè®¡ç®—ç¨³å¥æ ‡å‡†å·®"""
        n = len(data)
        location = np.median(data)
        
        # åˆå§‹å°ºåº¦ä¼°è®¡
        mad = np.median(np.abs(data - location))
        if mad == 0:
            q90, q10 = np.percentile(data, [90, 10])
            scale = (q90 - q10) / (2 * 1.645)
            if scale == 0:
                q75, q25 = np.percentile(data, [75, 25])
                iqr = q75 - q25
                scale = iqr / 1.349 if iqr > 0 else np.std(data) * 0.1
        else:
            scale = 1.4826 * mad
        
        # è¿­ä»£è®¡ç®—
        for i in range(max_iter):
            residuals = data - location
            standardized = residuals / scale
            
            psi_values = np.where(np.abs(standardized) <= c, 
                                 standardized, 
                                 c * np.sign(standardized))
            
            new_location = location + scale * np.mean(psi_values)
            
            chi_values = np.where(np.abs(standardized) <= c, 
                                 standardized**2, 
                                 c**2)
            
            new_scale = scale * np.sqrt(np.mean(chi_values) / 0.5)
            
            if (abs(new_location - location) < tol and 
                abs(new_scale - scale) < tol):
                break
                
            location, scale = new_location, new_scale
        
        return location, scale
    
    def comprehensive_analysis(self, data, c):
        """ç»¼åˆåˆ†æ"""
        # ä¼ ç»Ÿç»Ÿè®¡é‡
        traditional_std = np.std(data)
        mad_std = 1.4826 * np.median(np.abs(data - np.median(data)))
        
        # Huberç¨³å¥ä¼°è®¡
        huber_location, huber_std = self.huber_robust_std(data, c)
        
        # æ•°æ®ç‰¹å¾
        characteristics = self.analyze_data_characteristics(data)
        
        results = {
            'traditional_std': traditional_std,
            'mad_std': mad_std if mad_std > 0 else 0,
            'huber_std': huber_std,
            'huber_location': huber_location,
            'data_type': characteristics['data_type'],
            'characteristics': characteristics
        }
        
        self.analysis_results.update(results)
        return results

def plot_data_distribution(data, huber_location, data_type):
    """ç»˜åˆ¶æ•°æ®åˆ†å¸ƒå›¾"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # ç›´æ–¹å›¾
    ax1.hist(data, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
    ax1.axvline(huber_location, color='red', linestyle='--', 
                label=f'Huberä½ç½®: {huber_location:.3f}')
    ax1.set_xlabel('æ•°å€¼')
    ax1.set_ylabel('é¢‘æ•°')
    ax1.set_title(f'æ•°æ®åˆ†å¸ƒ - {data_type}')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # ç®±çº¿å›¾
    ax2.boxplot(data, vert=True)
    ax2.set_ylabel('æ•°å€¼')
    ax2.set_title('æ•°æ®ç®±çº¿å›¾')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

def get_sample_data(choice):
    """è·å–ç¤ºä¾‹æ•°æ®"""
    if choice == "æ•°æ®ä¸€ï¼ˆé«˜åº¦é›†ä¸­ï¼‰":
        return np.array([
            -21, -20, -20, -20, -20, -20, -20, -20, -20, -20, -20, -20, -20, -20, -20,
            -20, -20, -20, -20, -20, -20, -20, -20, -20, -20, -20, -20, -20, -19, -19,
            -19, -19, -19, -19, -19, -19, -19, -19, -19, -19, -19, -19, -19, -19, -19,
            -19, -19, -19, -19, -19, -19, -19, -19, -19, -19, -19, -19, -19, -19, -19,
            -19, -19, -19, -19, -19, -19, -18
        ])
    else:  # æ•°æ®äºŒï¼ˆä¸­ç­‰é›†ä¸­ï¼‰
        return np.array([
            827.6, 827.6, 827.6, 827.7, 827.7, 827.7, 827.6, 827.6, 827.6, 827.6,
            827.6, 827.7, 827.6, 827.6, 827.7, 827.6, 827.6, 827.7, 827.7, 827.7,
            827.7, 827.7, 827.6, 827.6, 827.6, 827.6, 827.7, 827.7, 827.7, 827.7,
            827.6, 827.5, 827.5, 827.5, 827.8, 827.6, 827.8, 827.9, 827.4, 827.4,
            827.8, 827.4, 827.7, 827.5, 827.5, 827.6, 827.4, 828.1, 827.4, 827.5,
            827.6, 827.7, 827.6, 827.4, 827.6, 827.4, 827.2, 827.4, 826.1, 826.8,
            827.5, 827.4, 827.6, 827.1, 827.4, 827.7
        ])

# ä¸»åº”ç”¨é€»è¾‘
def main():
    analyzer = RobustStdAnalyzer()
    data = None
    
    # æ•°æ®è¾“å…¥éƒ¨åˆ†
    if input_method == "ä¸Šä¼ CSVæ–‡ä»¶":
        uploaded_file = st.file_uploader("ä¸Šä¼ CSVæ–‡ä»¶", type=['csv'])
        if uploaded_file is not None:
            try:
                df = pd.read_csv(uploaded_file)
                st.write("æ•°æ®é¢„è§ˆ:", df.head())
                # å‡è®¾æ•°æ®åœ¨ç¬¬ä¸€åˆ—
                if len(df.columns) > 0:
                    data = df.iloc[:, 0].values
                    st.success(f"æˆåŠŸåŠ è½½ {len(data)} ä¸ªæ•°æ®ç‚¹")
            except Exception as e:
                st.error(f"æ–‡ä»¶è¯»å–é”™è¯¯: {e}")
    
    elif input_method == "ç›´æ¥è¾“å…¥æ•°æ®":
        data_input = st.text_area("è¾“å…¥æ•°æ®ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰:", 
                                value="-21, -20, -20, -20, -19, -19, -18")
        if st.button("è§£ææ•°æ®"):
            try:
                data_list = [float(x.strip()) for x in data_input.split(',')]
                data = np.array(data_list)
                st.success(f"æˆåŠŸè§£æ {len(data)} ä¸ªæ•°æ®ç‚¹")
            except Exception as e:
                st.error(f"æ•°æ®è§£æé”™è¯¯: {e}")
    
    else:  # ä½¿ç”¨ç¤ºä¾‹æ•°æ®
        sample_choice = st.selectbox("é€‰æ‹©ç¤ºä¾‹æ•°æ®é›†:", 
                                   ["æ•°æ®ä¸€ï¼ˆé«˜åº¦é›†ä¸­ï¼‰", "æ•°æ®äºŒï¼ˆä¸­ç­‰é›†ä¸­ï¼‰"])
        data = get_sample_data(sample_choice)
        st.success(f"å·²åŠ è½½ç¤ºä¾‹æ•°æ®: {sample_choice} ({len(data)} ä¸ªæ•°æ®ç‚¹)")
    
    # æ•°æ®åˆ†æéƒ¨åˆ†
    if data is not None:
        st.markdown("---")
        
        # æ‰§è¡Œåˆ†æ
        with st.spinner("æ­£åœ¨åˆ†ææ•°æ®..."):
            results = analyzer.comprehensive_analysis(data, c_value)
            characteristics = results['characteristics']
        
        # æ˜¾ç¤ºåˆ†æç»“æœ
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("æ•°æ®ç±»å‹", characteristics['data_type'])
            st.metric("æ•°æ®é‡", characteristics['n'])
            st.metric("å”¯ä¸€å€¼æ•°é‡", characteristics['unique_values_count'])
        
        with col2:
            st.metric("å‡å€¼", f"{characteristics['mean']:.4f}")
            st.metric("ä¸­ä½æ•°", f"{characteristics['median']:.4f}")
            st.metric("é›†ä¸­åº¦æ¯”ä¾‹", f"{characteristics['concentration_ratio']:.3f}")
        
        with col3:
            st.metric("ä¼ ç»Ÿæ ‡å‡†å·®", f"{results['traditional_std']:.4f}")
            st.metric("MADæ ‡å‡†å·®", f"{results['mad_std']:.4f}")
            st.metric("Huberç¨³å¥æ ‡å‡†å·®", f"{results['huber_std']:.4f}")
        
        # å¯è§†åŒ–
        st.markdown("### ğŸ“ˆ æ•°æ®å¯è§†åŒ–")
        fig = plot_data_distribution(data, results['huber_location'], 
                                   characteristics['data_type'])
        st.pyplot(fig)
        
        # è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
        st.markdown("### ğŸ“‹ è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯")
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**æ•°æ®åˆ†å¸ƒç»Ÿè®¡:**")
            unique_vals, counts = np.unique(data, return_counts=True)
            dist_df = pd.DataFrame({
                'æ•°å€¼': unique_vals,
                'é¢‘æ•°': counts,
                'æ¯”ä¾‹ (%)': (counts / len(data) * 100).round(2)
            })
            st.dataframe(dist_df, use_container_width=True)
        
        with col2:
            st.write("**åˆ†ä½æ•°ç»Ÿè®¡:**")
            quantiles = {
                'æœ€å°å€¼': np.min(data),
                'Q1 (25%)': np.percentile(data, 25),
                'ä¸­ä½æ•° (50%)': np.percentile(data, 50),
                'Q3 (75%)': np.percentile(data, 75),
                'æœ€å¤§å€¼': np.max(data),
                'IQR': characteristics['IQR']
            }
            quantile_df = pd.DataFrame(list(quantiles.items()), 
                                     columns=['ç»Ÿè®¡é‡', 'å€¼'])
            st.dataframe(quantile_df, use_container_width=True)
        
        # æ–¹æ³•æ¯”è¾ƒ
        st.markdown("### âš–ï¸ æ–¹æ³•æ¯”è¾ƒ")
        methods_data = {
            'æ–¹æ³•': ['ä¼ ç»Ÿæ ‡å‡†å·®', 'MADæ ‡å‡†å·®', 'Huberç¨³å¥æ ‡å‡†å·®'],
            'æ ‡å‡†å·®ä¼°è®¡': [
                results['traditional_std'],
                results['mad_std'],
                results['huber_std']
            ],
            'é€‚ç”¨åœºæ™¯': [
                'æ— å¼‚å¸¸å€¼çš„æ•°æ®',
                'æœ‰å¼‚å¸¸å€¼ä½†é«˜åº¦é›†ä¸­',
                'å„ç§æ•°æ®ç±»å‹ï¼ˆæ¨èï¼‰'
            ]
        }
        methods_df = pd.DataFrame(methods_data)
        st.dataframe(methods_df, use_container_width=True)
        
        # è§£é‡Šè¯´æ˜
        st.markdown("### ğŸ’¡ åˆ†æè¯´æ˜")
        st.info(f"""
        **æ•°æ®ç±»å‹è¯†åˆ«**: {characteristics['data_type']}
        
        - **ä¼ ç»Ÿæ ‡å‡†å·®**: {results['traditional_std']:.4f} - å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ
        - **MADæ ‡å‡†å·®**: {results['mad_std']:.4f} - å¯¹å¼‚å¸¸å€¼ç¨³å¥ï¼Œä½†å¯èƒ½ä¸º0
        - **Huberç¨³å¥æ ‡å‡†å·®**: {results['huber_std']:.4f} - å¹³è¡¡ç¨³å¥æ€§å’Œæ•ˆç‡
        
        **æ¨è**: å¯¹äº{characteristics['data_type']}ï¼Œå»ºè®®ä½¿ç”¨Huberç¨³å¥æ ‡å‡†å·®ä½œä¸ºå˜å¼‚æ€§åº¦é‡ã€‚
        """)
        
        # ä¸‹è½½ç»“æœ
        st.markdown("### ğŸ“¥ ä¸‹è½½åˆ†æç»“æœ")
        results_df = pd.DataFrame({
            'ç»Ÿè®¡é‡': [
                'æ•°æ®é‡', 'å‡å€¼', 'ä¸­ä½æ•°', 'ä¼ ç»Ÿæ ‡å‡†å·®', 'MADæ ‡å‡†å·®', 
                'Huberç¨³å¥æ ‡å‡†å·®', 'æ•°æ®ç±»å‹', 'é›†ä¸­åº¦æ¯”ä¾‹'
            ],
            'å€¼': [
                characteristics['n'], characteristics['mean'], 
                characteristics['median'], results['traditional_std'],
                results['mad_std'], results['huber_std'],
                characteristics['data_type'], characteristics['concentration_ratio']
            ]
        })
        
        csv = results_df.to_csv(index=False)
        b64 = base64.b64encode(csv.encode()).decode()
        href = f'<a href="data:file/csv;base64,{b64}" download="robust_std_analysis.csv">ä¸‹è½½CSVåˆ†ææŠ¥å‘Š</a>'
        st.markdown(href, unsafe_allow_html=True)

# è¿è¡Œåº”ç”¨
if __name__ == "__main__":
    main()  